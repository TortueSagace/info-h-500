<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<link href="../../img/favicon.ico" rel="shortcut icon"/>
<title>Image sensors - INFO-H-500 Image Acquisition &amp; Processing</title>
<link href="../../css/bootstrap.min.css" rel="stylesheet"/>
<link href="../../css/font-awesome.min.css" rel="stylesheet"/>
<link href="../../css/base.css" rel="stylesheet"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" rel="stylesheet"/>
<script defer="" src="../../js/jquery-1.10.2.min.js"></script>
<script defer="" src="../../js/bootstrap.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/yaml.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/django.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
</head>
<body>
<div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
<div class="container">
<a class="navbar-brand" href="../../index.html">INFO-H-500 Image Acquisition &amp; Processing</a>
<!-- Expander button -->
<button class="navbar-toggler" data-target="#navbar-collapse" data-toggle="collapse" type="button">
<span class="navbar-toggler-icon"></span>
</button>
<!-- Expanded navigation -->
<div class="navbar-collapse collapse" id="navbar-collapse">
<!-- Main navigation -->
<ul class="nav navbar-nav">
<li class="navitem">
<a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="dropdown active">
<a class="nav-link dropdown-toggle" data-toggle="dropdown" href="#">1.Introduction <b class="caret"></b></a>
<ul class="dropdown-menu">
<li>
<a class="dropdown-item" href="../01/01-Biological_vision.html">Biological vision</a>
</li>
<li>
<a class="dropdown-item active" href="02-Image_sensors.html">Image sensors</a>
</li>
<li>
<a class="dropdown-item" href="../03/03-Digital_image.html">Image representation</a>
</li>
</ul>
</li>
<li class="dropdown">
<a class="nav-link dropdown-toggle" data-toggle="dropdown" href="#">2.Low-level image processing <b class="caret"></b></a>
<ul class="dropdown-menu">
<li>
<a class="dropdown-item" href="../../Low-level_image_processing/01-Point_processing.html">Histogram</a>
</li>
<li>
<a class="dropdown-item" href="../../Low-level_image_processing/02-Linear_filtering.html">Linear filtering</a>
</li>
<li>
<a class="dropdown-item" href="../../Low-level_image_processing/03-Non_linear_filtering.html">Rank based filters</a>
</li>
<li>
<a class="dropdown-item" href="../../Low-level_image_processing/04-Image_restoration.html">Image restoration</a>
</li>
<li>
<a class="dropdown-item" href="../../Low-level_image_processing/05-Edge_detection.html">Edge detection</a>
</li>
</ul>
</li>
<li class="dropdown">
<a class="nav-link dropdown-toggle" data-toggle="dropdown" href="#">3.Image segmentation <b class="caret"></b></a>
<ul class="dropdown-menu">
<li>
<a class="dropdown-item" href="../../Image_segmentation/00-Image_processing_chain.html">Typical image processing pipelines</a>
</li>
<li>
<a class="dropdown-item" href="../../Image_segmentation/01-Histogram_based_image_segmentation.html">Histogram based segmentation</a>
</li>
<li>
<a class="dropdown-item" href="../../Image_segmentation/02-Border-based_segmentation.html">Border based segmentation</a>
</li>
<li>
<a class="dropdown-item" href="../../Image_segmentation/03-Region_based_segmentation.html">Region based segmentation</a>
</li>
<li class="dropdown-submenu">
<a class="dropdown-item" href="#">Model based segmentation</a>
<ul class="dropdown-menu">
<li>
<a class="dropdown-item" href="../../Image_segmentation/04-Live-wire.html">Model based segmentation</a>
</li>
<li>
<a class="dropdown-item" href="../../Image_segmentation/05-Active_contour.html">Active contours</a>
</li>
<li>
<a class="dropdown-item" href="../../Image_segmentation/06-Hough_transform.html">Hough transform</a>
</li>
</ul>
</li>
<li>
<a class="dropdown-item" href="../../Image_segmentation/07-Examples.html">Examples</a>
</li>
</ul>
</li>
<li class="dropdown">
<a class="nav-link dropdown-toggle" data-toggle="dropdown" href="#">4.Morphomathematics <b class="caret"></b></a>
<ul class="dropdown-menu">
<li>
<a class="dropdown-item" href="../../Morphomathematics/01-Operators.html">Morphomathematical operators</a>
</li>
<li>
<a class="dropdown-item" href="../../Morphomathematics/02-Combined_operations.html">Combined operations</a>
</li>
<li>
<a class="dropdown-item" href="../../Morphomathematics/03-Watershed_transform.html">The watershed transform</a>
</li>
<li>
<a class="dropdown-item" href="../../Morphomathematics/04-Graylevel_morphology.html">Gray level morphology</a>
</li>
</ul>
</li>
<li class="navitem">
<a class="nav-link" href="../../labs.html">LABS</a>
</li>
<li class="navitem">
<a class="nav-link" href="../../references.html">References</a>
</li>
<li class="navitem">
<a class="nav-link" href="../../about.html">About</a>
</li>
</ul>
<ul class="nav navbar-nav ml-auto">
<li class="nav-item">
<a class="nav-link" data-target="#mkdocs_search_modal" data-toggle="modal" href="#">
<i class="fa fa-search"></i> Search
                            </a>
</li>
<li class="nav-item">
<a class="nav-link" href="../01/01-Biological_vision.html" rel="prev">
<i class="fa fa-arrow-left"></i> Previous
                                </a>
</li>
<li class="nav-item">
<a class="nav-link" href="../03/03-Digital_image.html" rel="next">
                                    Next <i class="fa fa-arrow-right"></i>
</a>
</li>
<li class="nav-item">
<a class="nav-link" href="https://github.com/odebeir/test_mkdocs"><i class="fa fa-github"></i> GitHub</a>
</li>
</ul>
</div>
</div>
</div>
<div class="container">
<div class="row">
<div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
<div class="navbar-header">
<button class="navbar-toggler collapsed" data-target="#toc-collapse" data-toggle="collapse" title="Table of Contents" type="button">
<span class="fa fa-angle-down"></span>
</button>
</div>
<div class="navbar-collapse collapse card bg-secondary" id="toc-collapse">
<ul class="nav flex-column">
<li class="nav-item" data-level="1"><a class="nav-link" href="#image-sensors">Image Sensors</a>
<ul class="nav flex-column">
<li class="nav-item" data-level="2"><a class="nav-link" href="#passive-vs-active-imaging">Passive vs active imaging</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-level="2"><a class="nav-link" href="#direct-image-acquisition">Direct image acquisition</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-level="2"><a class="nav-link" href="#indirect-image-acquisition">Indirect image acquisition</a>
<ul class="nav flex-column">
</ul>
</li>
<li class="nav-item" data-level="2"><a class="nav-link" href="#synthetic-images">Synthetic images</a>
<ul class="nav flex-column">
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div></div>
<div class="col-md-9" role="main">
<h1 id="image-sensors">Image Sensors<a class="headerlink" href="#image-sensors" title="Permanent link"></a></h1>
<h2 id="passive-vs-active-imaging">Passive vs active imaging<a class="headerlink" href="#passive-vs-active-imaging" title="Permanent link"></a></h2>
<ul>
<li>the object is the source of photon (SPECT, stars,...)</li>
<li>the object reflects/react to light given by a external source (flash, fluorescence)</li>
<li>the object is traversed by the ligh and diffuses/asborbes it (X-ray)</li>
</ul>
<p><img alt="png" src="output_3_0.png"/></p>
<h3 id="object-as-a-source">Object as a source<a class="headerlink" href="#object-as-a-source" title="Permanent link"></a></h3>
<p>Nuclear imaging is a good example of the first setup, here an injection of radio-tracer will accumulates to some region of interset (due to specific biochemical affinity). The following example shows how the radio-tracer identifies bone metastasis of a prostate cancer using a gamma camera.</p>
<p><img alt="jpeg" src="output_5_0.jpg"/></p>
<p><sup><a href="https://commons.wikimedia.org/wiki/File:Prostate-mets-102.jpg">wikimedia commons</a><sup></sup></sup></p>
<p>The source can also be the result of an external exitation i.e. an absorbtion and a re-emission of an other photon (fluorescence).</p>
<p>Fluorescence lymphography is an example of imaging using an external exitation, here, infrared light is used to exite fluorophore injected in the lymph system. Fluorophore can in turn re-emit infrared (at a longer wavelength). By using adapted filter, one can observe the lymph displacement inside the lymph network (close to the skin surface).</p>
<p><img alt="png" src="output_8_0.png"/></p>
<p><sup>J.P.Belgrado<sup></sup></sup></p>
<p>An other example, where fluorescence is used: the fluorescence microscopy.</p>
<h3 id="object-reflects-diffuses-the-light-from-an-external-source">Object reflects / diffuses the light from an external source<a class="headerlink" href="#object-reflects-diffuses-the-light-from-an-external-source" title="Permanent link"></a></h3>
<p>This is the more common acquisition setup, external light source flood the scene with visible photons that are reflected by the objects, these photons are then acquired by a sensor.</p>
<h3 id="object-attenuates-the-source">Object attenuates the source<a class="headerlink" href="#object-attenuates-the-source" title="Permanent link"></a></h3>
<p>Source and sensor can be placed on both side of the object being imaged, a good example is the X-Ray imaging, where a X-Ray source project photon trough a patient, these photon interact with the matter in such a way that tissue density and composition (bones vs soft tissues) can give a contrast variation at the sensor level.</p>
<p><img alt="jpeg" src="output_14_0.jpg"/></p>
<p><sup><a href="https://en.wikipedia.org/wiki/X-ray#/media/File:Lung_X-ray.jpg">image source</a><sup></sup></sup></p>
<h2 id="direct-image-acquisition">Direct image acquisition<a class="headerlink" href="#direct-image-acquisition" title="Permanent link"></a></h2>
<h3 id="ccd-coupled-charge-device">CCD - coupled charge device<a class="headerlink" href="#ccd-coupled-charge-device" title="Permanent link"></a></h3>
<p>Charges are liberated by light interaction with the semiconductor inside photoactive region, for each pixel of the sensor grid. In order to digitize the amount of charges (proportinal to light captured, CCD devices will move the charges along the substrate up to a charge to voltage converter.</p>
<p>Coupled Charge Device uses electrode potentials to move charges inside silicium substrate as illustrated bellow.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/6/66/CCD_charge_transfer_animation.gif"/></p>
<p><sup><a href="https://commons.wikimedia.org/wiki/File:CCD_charge_transfer_animation.gif">wikimedia commons</a><sup></sup></sup></p>
<p>Image sensor can have essentially two types of geometry: </p>
<ul>
<li>linear: typically used when the sensor is translated (flatbed scanner, but also bank note scanner, satellite, photo finish)</li>
<li>rectangular: almost every other camera</li>
</ul>
<p>In order to move charges along the dimensions of the CCD sensor, charges are moved along each image line, a perpendicular buffer is the used to discharge all these pixels in column into an amplifier that transform each charge into a voltage. The voltage is then converted by an ADC circuit. </p>
<p>Because all the pixels charges are compared using the same circuit, the CCD sensor provide a very constant specification on the complete sensor. The other main advantage of the sensor is the coverage factor of the sensor (the ratio between the sensor surface and the total pixel surface), almost the surface is devoted to light acquisition (no extra circuitry needed).</p>
<p><img alt="png" src="output_22_0.png"/></p>
<p><sup><a href="https://commons.wikimedia.org/wiki/File%3ACcd_schematic.JPG">wikimedia commons</a><sup></sup></sup></p>
<p><img alt="jpeg" src="output_24_0.jpg"/></p>
<p>Linear CCD sensor.
<sup><a href="https://commons.wikimedia.org/wiki/File:CCD_line_sensor.JPG">wikimedia commons</a><sup></sup></sup></p>
<p><img alt="jpeg" src="output_26_0.jpg"/></p>
<p>CCD line sensor in a ceramic dual in-line package. 
<sup><a href="https://commons.wikimedia.org/wiki/File:CCD_in_camera.jpg">wikimedia commons</a><sup></sup></sup></p>
<h3 id="cmos">CMOS<a class="headerlink" href="#cmos" title="Permanent link"></a></h3>
<p>The CMOS technology embeds a photo-detector and a charge amplifier for each sensor pixel, the voltage being then transmitted by electrical conductors.</p>
<p>This strategy enables a greater variety of sensor usage, e.g. adressing a part of the senor (for low resolution and higher speed).</p>
<p>Because the conversion is done separately for each pixels, no charge shifting is needed, but discrepency between charge amplifier may exist, giving unequal pixel sensitivity and noise.</p>
<p><img alt="png" src="output_29_0.png"/></p>
<p><sup><a href="https://commons.wikimedia.org/wiki/File:CMOS_Image_Sensor_Mechanism_Illustration.svg">wikimedia commons</a><sup></sup></sup></p>
<h3 id="cmos-vs-ccd">CMOS vs CCD<a class="headerlink" href="#cmos-vs-ccd" title="Permanent link"></a></h3>
<table>
<thead>
<tr>
<th>feature</th>
<th>CCD</th>
<th>CMOS</th>
</tr>
</thead>
<tbody>
<tr>
<td>Signal out of pixel</td>
<td>Electron packet</td>
<td>Voltage</td>
</tr>
<tr>
<td>Fill factor</td>
<td>high</td>
<td>moderate</td>
</tr>
<tr>
<td>Amplifier mismatch</td>
<td>none</td>
<td>moderate</td>
</tr>
<tr>
<td>Noise</td>
<td>low</td>
<td>moderate</td>
</tr>
<tr>
<td>system complexity</td>
<td>high</td>
<td>low</td>
</tr>
<tr>
<td>sensor complexity</td>
<td>low</td>
<td>high</td>
</tr>
<tr>
<td>dynamic range</td>
<td>high</td>
<td>moderate</td>
</tr>
<tr>
<td>uniformity</td>
<td>high</td>
<td>moderate</td>
</tr>
<tr>
<td>speed</td>
<td>moderate</td>
<td>high</td>
</tr>
</tbody>
</table>
<p>CMOS + CCD : high sensitivity to near infrared, therefore, most of the sensors are equiped with a NIR filter. </p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Response_silicon_photodiode.svg/544px-Response_silicon_photodiode.svg.png"/></p>
<p><sup><a href="https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Response_silicon_photodiode.svg/544px-Response_silicon_photodiode.svg.png">wikimedia commons</a><sup></sup></sup></p>
<h3 id="multispectral-acquisition">Multispectral acquisition<a class="headerlink" href="#multispectral-acquisition" title="Permanent link"></a></h3>
<p>Color acquisition is done by acquiring several images at different wavelength, one common (and cheap) approach is to cover sensors pixels by colored dyes (red, green and blue). The figure above illustrated such filters (bayer), where on each 2x2 pixel square, one pixel is sensitive to the red part of the spectrum, one to the blue part of the spectrum, and finally 2 pixels sensitive to the green part of the spectrum.</p>
<p>The choice of duplicating green is done for symetry purposes and also because the intensity sensitivity of the eye (see rods) is correlated to the green part of the spectrum.</p>
<p><img alt="png" src="output_36_0.png"/></p>
<p><sup><a href="https://en.wikipedia.org/wiki/Bayer_filter#/media/File:Bayer_pattern_on_sensor.svg">wiki commons</a><sup></sup></sup></p>
<p>One limitation of the dye approach is the resolution limitation, indeed the image resolution is divided by 4.</p>
<p>The other method used is based on three CCD coupled on the same optical axis and having three different dyes (red, green, blue) as illustrated bellow.</p>
<p><img alt="png" src="output_39_0.png"/></p>
<p><sup><a href="https://commons.wikimedia.org/wiki/File:Dichroic-prism.svg">wikimedia commons</a><sup></sup></sup></p>
<p>The big advantage of this approach is to keep the sensor native resolution for each color channel.</p>
<p>The number of spectral bands can be higher that three, for example satellite imagery offers many wavelength inside but also next to it (UV and near-IR).</p>
<p>Quick-bird (envionemental imagery,  pixel = 0.65m)
* Pan: 450-900 nm
* Blue: 450-520 nm
* Green: 520-600 nm
* Red: 630-690 nm
* Near IR: 760-900 nm</p>
<p>IKONOS (commercial earth observation satellite)</p>
<p>resolution</p>
<ul>
<li>0.8 m panchromatic (1-m PAN)</li>
<li>4-meter multispectral (4-m MS)</li>
</ul>
<p>spectrum</p>
<ul>
<li>Blue: 0.445–0.516 µm</li>
<li>Green: 0.506–0.595 µm</li>
<li>Red: 0.632–0.698 µm</li>
<li>Near IR: 0.757–0.853 µm</li>
</ul>
<p>Landsat 8 (American Earth observation satellite)</p>
<ul>
<li>Band 1 - Coastal / Aerosol    0.433 - 0.453 µm    30 m</li>
<li>Band 2 - Blue 0.450 - 0.515 µm    30 m</li>
<li>Band 3 - Green    0.525 - 0.600 µm    30 m</li>
<li>Band 4 - Red  0.630 - 0.680 µm    30 m</li>
<li>Band 5 - Near Infrared    0.845 - 0.885 µm    30 m</li>
<li>Band 6 - Short Wavelength Infrared    1.560 - 1.660 µm    30 m</li>
<li>Band 7 - Short Wavelength Infrared    2.100 - 2.300 µm    30 m</li>
<li>Band 8 - Panchromatic 0.500 - 0.680 µm    15 m</li>
<li>Band 9 - Cirrus   1.360 - 1.390 µm    30 m</li>
</ul>
<p>AVIRIS - airborne visible/infrared imaging spectrometer 
* four linear spectrometers (614-pixel wide) / 224 adjacent spectral bands.</p>
<p><img alt="jpeg" src="output_42_0.jpg"/></p>
<p><sup><a href="https://commons.wikimedia.org/wiki/File:HyperspectralCube.jpg">wikimedia commons</a><sup></sup></sup></p>
<h3 id="depth-acquisition">Depth acquisition<a class="headerlink" href="#depth-acquisition" title="Permanent link"></a></h3>
<p>Depth imaging is traditionnaly used in stereo application, such for robot vision.
Recently depth sensor became widely available thanks to game applications.The main technologies used are:
* stereovision 
* laser triangulation
* structured light projection
* Time-Of-Flight (TOF) imaging</p>
<p>The information provided by these sensors is of two types: a rgb image of the scene, and a depth estimation (usually at a coarser resolution).</p>
<p>example of a high resolution laser triangulation scanner:</p>
<p><img alt="png" src="output_46_0.png"/></p>
<p>When high speed is needed, structured light may be a solution.</p>
<p>For example, the first generation of the Kinect sensor uses the principe of structured light projection, a pseudo-random pattern is projected in the near-infrared spectrum(i.e. invible to human eye) and acquired by a IR sensitive camera. The depth image is produced with a video framerate compatible with gaming.</p>
<p><img alt="png" src="output_48_0.png"/></p>
<p><sup><a href="https://commons.wikimedia.org/wiki/File:Xbox-360-Kinect-Standalone.png">wikimedia commons</a><sup></sup></sup></p>
<p><img alt="jpeg" src="output_50_0.jpg"/></p>
<p><sup><a href="https://www.mattcutts.com/blog/open-kinect-contest/">image source</a><sup></sup></sup></p>
<p><img alt="jpeg" src="output_52_0.jpg"/></p>
<p><sup><a href="http://image-sensors-world.blogspot.be/2010_11_01_archive.html">image source</a><sup></sup></sup></p>
<p>The depth is computed by triangulation thanks to the identification of specific pattern in the image.</p>
<p>The second generation of sensors is based on a completely different technology, the Time-Of-Flight (TOF). To estimate the distance between the sensor and the scene, a light wave is send and received by the sensor. The phase difference between a modulated light pattern sended by the source and the signal received by the camera gives a measure of the scene depth.</p>
<p>How to measure distance with light ?</p>
<p><img src="http://www.laserfocusworld.com/content/dam/etc/medialib/new-lib/laser-focus-world/online-articles/2011/01/98070.res/_jcr_content/renditions/pennwell.web.399.293.gif"/></p>
<p><sup><a href="http://www.laserfocusworld.com/articles/2011/01/lasers-bring-gesture-recognition-to-the-home.html">image source</a><sup></sup></sup></p>
<p>Continuous wave demodulation</p>
<ul>
<li>retrieve phase shift by demodulation of the received signal</li>
<li>demodulation by cross-correlation of the received signal with the emitted signal</li>
<li>emitted signal is
$$g(t) = \cos(\omega t)$$ with $\omega$ the modulation frequency</li>
<li>received signal after the return trip to the scene surface:
$$s(t) = b + a \cos(\omega t +\phi)$$ where $a$ is an unknown attenuation, $\phi$ the phase shift <strong>i.e. a value proportional to the scene distance</strong> and $b$ an unknown acquisition noise (neglected here).</li>
</ul>
<p><img alt="png" src="output_61_0.png"/></p>
<ul>
<li>cross correlation of both emitted and received signal becomes:
$$ d(\tau) = s * g = \int_{-\inf}^{+\inf} s(t).g(t+\tau) dt$$ with $\tau$ an internal offset</li>
</ul>
<p>$$ d(\tau) = \frac a 2 \cos(\omega t + \phi) + b $$</p>
<ul>
<li>sample $d(\tau)$ at 4 distinct moments (phase offsets):
$$A_i = d(i.\frac \pi 2)  \text{ with } i = 0,\dots, 3$$</li>
</ul>
<p><img alt="png" src="output_64_0.png"/></p>
<ul>
<li>phase and attenuation are then:
$$ \phi = \arg \tan(\frac{A_3-A_1}{A_0-A_2}) $$
and
$$ a = \frac 1 2 \sqrt{(A_3-A_1)^2+(A_0-A_2)^2}$$</li>
<li>scene distance is then:
$$dist = \frac{c}{4.\pi.\omega} \phi$$ where $c$ is the speed of light.</li>
</ul>
<p>What a depth image looks like ?</p>
<p><img alt="jpeg" src="output_67_0.jpg"/></p>
<p><sup><a href="https://commons.wikimedia.org/wiki/File:TOF_Kamera_3D_Gesicht.jpg">wikimedia commons</a><sup></sup></sup></p>
<h2 id="indirect-image-acquisition">Indirect image acquisition<a class="headerlink" href="#indirect-image-acquisition" title="Permanent link"></a></h2>
<p>Image can also be the result of a mathematical reconstruction based on an indirect acquisition, the sensor do not acquire an image directly.</p>
<p>For example, computed tomography, uses a series of 1D density profile acquisition enable a 2D reconstruction of the slice.</p>
<p><img alt="jpeg" src="output_70_0.jpg"/></p>
<p><sup><a href="" title="http://130.237.83.53/medicaldevices/album/Ch%207%20Medical%20images/slides/F%207-10%20Computer%20tomography.jpg">image source</a><sup></sup></sup></p>
<p>Echography is an other example of indirect imaging, where mechanical wave propagation are transformed in a 2D image showing the presence of interfaces between tissue of different acoustic impedence.</p>
<p><sup><a href="" title="https://en.wikipedia.org/wiki/Medical_ultrasound#/media/File:Ultrasound_of_human_heart_apical_4-cahmber_view.gif">wiki commons</a><sup></sup></sup></p>
<p>other example: MRI image reconstruction</p>
<h2 id="synthetic-images">Synthetic images<a class="headerlink" href="#synthetic-images" title="Permanent link"></a></h2>
<p>Image can also be the result of the grouping of a huge number of localized data, for example, one can imagine a network of temperature sensors spread over a complete contry, then the temperature measurements can be grouped on a 2D map (and interpolated to have a complete coverage).</p>
<p>Visualized data can be from various nature, the common aspect is that these data are placed in a geometric space (usually 2D or 3D). </p></div>
</div>
</div>
<footer class="col-md-12">
<hr/>
<p><a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" rel="license"><img alt="Creative Commons License" src="http://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" style="border-width:0"/> maintained by </a>Olivier Debeir and <a href="">contributors</a>.</p>
<p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
</footer>
<script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
<script defer="" src="../../js/base.js"></script>
<script defer="" src="../../search/main.js"></script>
<div aria-hidden="true" aria-labelledby="searchModalLabel" class="modal" id="mkdocs_search_modal" role="dialog" tabindex="-1">
<div class="modal-dialog modal-lg">
<div class="modal-content">
<div class="modal-header">
<h4 class="modal-title" id="searchModalLabel">Search</h4>
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true">×</span><span class="sr-only">Close</span></button>
</div>
<div class="modal-body">
<p>From here you can search these documents. Enter your search terms below.</p>
<form>
<div class="form-group">
<input class="form-control" id="mkdocs-search-query" placeholder="Search..." title="Type search term here" type="search"/>
</div>
</form>
<div data-no-results-text="No results found" id="mkdocs-search-results"></div>
</div>
<div class="modal-footer">
</div>
</div>
</div>
</div><div aria-hidden="true" aria-labelledby="keyboardModalLabel" class="modal" id="mkdocs_keyboard_modal" role="dialog" tabindex="-1">
<div class="modal-dialog">
<div class="modal-content">
<div class="modal-header">
<h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
<button class="close" data-dismiss="modal" type="button"><span aria-hidden="true">×</span><span class="sr-only">Close</span></button>
</div>
<div class="modal-body">
<table class="table">
<thead>
<tr>
<th style="width: 20%;">Keys</th>
<th>Action</th>
</tr>
</thead>
<tbody>
<tr>
<td class="help shortcut"><kbd>?</kbd></td>
<td>Open this help</td>
</tr>
<tr>
<td class="next shortcut"><kbd>n</kbd></td>
<td>Next page</td>
</tr>
<tr>
<td class="prev shortcut"><kbd>p</kbd></td>
<td>Previous page</td>
</tr>
<tr>
<td class="search shortcut"><kbd>s</kbd></td>
<td>Search</td>
</tr>
</tbody>
</table>
</div>
<div class="modal-footer">
</div>
</div>
</div>
</div>
</body>
</html>
